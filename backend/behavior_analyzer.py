"""
üß† Behavior Analyzer v2 - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏à‡∏≤‡∏Å Pose Estimation
‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö:
  - Multi-signal scoring ‡πÅ‡∏ó‡∏ô single threshold
  - ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå‡∏à‡∏≤‡∏Å wrist position
  - CLAHE preprocessing ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏á‡∏™‡∏ß‡πà‡∏≤‡∏á‡πÑ‡∏°‡πà‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠
  - ‡πÉ‡∏ä‡πâ yolov8s-pose (small) ‡πÅ‡∏ó‡∏ô nano ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
  - Calibrate threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏°‡∏∏‡∏°‡∏™‡∏π‡∏á‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö
"""
from ultralytics import YOLO
import numpy as np
import cv2
import os

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‡πÇ‡∏°‡πÄ‡∏î‡∏• (lazy-loading)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
pose_model = None
_MODEL_NAME = "yolov8s-pose.pt"   # small > nano (‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)

def get_pose_model():
    global pose_model
    if pose_model is None:
        base = os.path.dirname(os.path.abspath(__file__))
        small_path = os.path.join(base, _MODEL_NAME)
        nano_path  = os.path.join(base, "yolov8n-pose.pt")
        if os.path.exists(small_path):
            pose_model = YOLO(small_path)
        elif os.path.exists(nano_path):
            print("‚ö†Ô∏è  yolov8s-pose.pt ‡πÑ‡∏°‡πà‡∏û‡∏ö ‚Äî ‡πÉ‡∏ä‡πâ yolov8n-pose.pt ‡πÅ‡∏ó‡∏ô")
            pose_model = YOLO(nano_path)
        else:
            pose_model = YOLO(_MODEL_NAME)   # ‡πÉ‡∏´‡πâ ultralytics ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏á
    return pose_model


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Preprocessing
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def preprocess_frame(frame: np.ndarray) -> np.ndarray:
    """CLAHE ‡∏ö‡∏ô L-channel ‡πÄ‡∏û‡∏∑‡πà‡∏≠ normalize ‡πÅ‡∏™‡∏á‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡πÉ‡∏ô‡∏´‡πâ‡∏≠‡∏á‡πÄ‡∏£‡∏µ‡∏¢‡∏ô"""
    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    l = clahe.apply(l)
    return cv2.cvtColor(cv2.merge([l, a, b]), cv2.COLOR_LAB2BGR)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Keypoint helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
KP_CONF_THRESHOLD = 0.40   # keypoint confidence ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠

_KP_IDX = dict(
    nose=0,
    left_eye=1,  right_eye=2,
    left_ear=3,  right_ear=4,
    left_shoulder=5,  right_shoulder=6,
    left_elbow=7,     right_elbow=8,
    left_wrist=9,     right_wrist=10,
    left_hip=11,      right_hip=12,
)

def _get(kp, name):
    """‡∏Ñ‡∏∑‡∏ô (x, y) ‡∏ñ‡πâ‡∏≤ confidence ‡∏ú‡πà‡∏≤‡∏ô threshold, ‡πÑ‡∏°‡πà‡∏á‡∏±‡πâ‡∏ô None"""
    idx = _KP_IDX.get(name)
    if idx is None or idx >= len(kp):
        return None
    x, y, c = kp[idx]
    return np.array([float(x), float(y)]) if c >= KP_CONF_THRESHOLD else None

def _midpoint(a, b):
    return (a + b) / 2 if (a is not None and b is not None) else None

def _dist(a, b):
    return float(np.linalg.norm(a - b)) if (a is not None and b is not None) else None


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Multi-signal scoring
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _score_behavior(kp) -> dict:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì score ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏û‡∏§‡∏ï‡∏¥‡∏Å‡∏£‡∏£‡∏°‡∏à‡∏≤‡∏Å keypoints ‡∏´‡∏•‡∏≤‡∏¢‡∏à‡∏∏‡∏î

    Signals ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:
      S1  Head elevation ratio   (‡∏à‡∏°‡∏π‡∏Å vs ‡πÑ‡∏´‡∏•‡πà)
      S2  Trunk uprightness      (‡πÑ‡∏´‡∏•‡πà vs ‡∏™‡∏∞‡πÇ‡∏û‡∏Å)
      S3  Eye separation ratio   (‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏£‡∏á vs ‡∏´‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏á)
      S4  Ear symmetry           (‡πÄ‡∏´‡πá‡∏ô‡∏™‡∏≠‡∏á‡∏´‡∏π vs ‡∏´‡∏π‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
      S5  Wrist-to-face proximity (‡∏ñ‡∏∑‡∏≠‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå)
      S6  Elbow raised           (‡∏ß‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏®‡∏≠‡∏Å‡∏™‡∏π‡∏á)
    """
    scores = {"attentive": 0.0, "looking_down": 0.0,
              "sleeping": 0.0, "looking_away": 0.0}

    nose           = _get(kp, "nose")
    left_eye       = _get(kp, "left_eye")
    right_eye      = _get(kp, "right_eye")
    left_shoulder  = _get(kp, "left_shoulder")
    right_shoulder = _get(kp, "right_shoulder")
    left_wrist     = _get(kp, "left_wrist")
    right_wrist    = _get(kp, "right_wrist")
    left_elbow     = _get(kp, "left_elbow")
    right_elbow    = _get(kp, "right_elbow")
    left_hip       = _get(kp, "left_hip")
    right_hip      = _get(kp, "right_hip")

    shoulder_center = _midpoint(left_shoulder, right_shoulder)
    hip_center      = _midpoint(left_hip, right_hip)
    shoulder_width  = (_dist(left_shoulder, right_shoulder) or 80.0)

    # ‚îÄ‚îÄ S1: Head elevation ratio ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # ‡∏Å‡∏•‡πâ‡∏≠‡∏á‡∏´‡πâ‡∏≠‡∏á‡πÅ‡∏•‡πá‡∏ö‡∏°‡∏±‡∏Å‡∏≠‡∏¢‡∏π‡πà‡∏™‡∏π‡∏á ~30-45¬∞ calibrate threshold ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞
    if nose is not None and shoulder_center is not None:
        head_elev  = shoulder_center[1] - nose[1]   # ‡∏ö‡∏ß‡∏Å = ‡∏´‡∏±‡∏ß‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏´‡∏•‡πà (‡∏õ‡∏Å‡∏ï‡∏¥)
        head_ratio = head_elev / shoulder_width

        if   head_ratio > 0.55:                     # ‡∏ô‡∏±‡πà‡∏á‡∏ï‡∏£‡∏á ‡∏´‡∏±‡∏ß‡∏ï‡∏±‡πâ‡∏á‡∏ä‡∏±‡∏î
            scores["attentive"]    += 2.5
        elif head_ratio > 0.30:                     # ‡∏ô‡∏±‡πà‡∏á‡∏ï‡∏£‡∏á‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£
            scores["attentive"]    += 1.5
            scores["looking_down"] += 0.5
        elif head_ratio > 0.10:                     # ‡∏Å‡πâ‡∏°‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            scores["looking_down"] += 2.5
        elif head_ratio > -0.10:                    # ‡∏Å‡πâ‡∏°‡∏°‡∏≤‡∏Å
            scores["looking_down"] += 1.5
            scores["sleeping"]     += 1.5
        else:                                       # ‡∏´‡∏±‡∏ß‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å / ‡∏´‡∏•‡∏±‡∏ö
            scores["sleeping"]     += 3.0

    # ‚îÄ‚îÄ S2: Trunk uprightness ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if shoulder_center is not None and hip_center is not None:
        trunk_dy = shoulder_center[1] - hip_center[1]  # ‡∏•‡∏ö = ‡πÑ‡∏´‡∏•‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏∞‡πÇ‡∏û‡∏Å (‡∏õ‡∏Å‡∏ï‡∏¥)
        if trunk_dy > 5:          # ‡πÑ‡∏´‡∏•‡πà‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏∞‡πÇ‡∏û‡∏Å = ‡πÇ‡∏¢‡πâ‡∏ï‡∏±‡∏ß‡∏°‡∏≤‡∏Å / ‡∏´‡∏•‡∏±‡∏ö
            scores["sleeping"]  += 2.0
        elif trunk_dy < -20:      # ‡∏ï‡∏±‡πâ‡∏á‡∏ï‡∏£‡∏á‡∏î‡∏µ
            scores["attentive"] += 1.0

    # ‚îÄ‚îÄ S3: Eye separation ratio ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    eye_dist = _dist(left_eye, right_eye)
    if eye_dist is not None:
        eye_ratio = eye_dist / shoulder_width
        if   eye_ratio > 0.25:   scores["attentive"]    += 2.0  # ‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏£‡∏á
        elif eye_ratio > 0.12:   scores["attentive"]    += 0.5
        else:                    scores["looking_away"] += 1.5  # ‡∏´‡∏±‡∏ô‡∏Ç‡πâ‡∏≤‡∏á

    # ‚îÄ‚îÄ S4: Ear symmetry ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    left_ear_conf  = float(kp[3][2]) if len(kp) > 3 else 0.0
    right_ear_conf = float(kp[4][2]) if len(kp) > 4 else 0.0
    both_ears = (left_ear_conf  >= KP_CONF_THRESHOLD and
                 right_ear_conf >= KP_CONF_THRESHOLD)
    one_ear   = ((left_ear_conf  >= KP_CONF_THRESHOLD) ^
                 (right_ear_conf >= KP_CONF_THRESHOLD))
    if both_ears: scores["attentive"]    += 1.0
    elif one_ear: scores["looking_away"] += 1.0

    # ‚îÄ‚îÄ S5: Wrist-to-face proximity (phone detection) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    face_center = nose if nose is not None else _midpoint(left_eye, right_eye)
    if face_center is not None:
        for wrist in [left_wrist, right_wrist]:
            if wrist is not None:
                wr = (_dist(wrist, face_center) or 9999) / shoulder_width
                if   wr < 0.6:   # wrist ‡πÉ‡∏Å‡∏•‡πâ‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏≤‡∏Å = ‡∏ñ‡∏∑‡∏≠‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå
                    scores["looking_down"] += 2.0
                    scores["attentive"]    -= 0.5
                elif wr < 1.0:
                    scores["looking_down"] += 0.5

    # ‚îÄ‚îÄ S6: Elbow raised ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    for elbow, shoulder in [(left_elbow, left_shoulder),
                            (right_elbow, right_shoulder)]:
        if elbow is not None and shoulder is not None:
            if shoulder[1] - elbow[1] > 10:   # ‡∏Ç‡πâ‡∏≠‡∏®‡∏≠‡∏Å‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏´‡∏•‡πà
                scores["looking_down"] += 0.5

    # ‡∏Ñ‡∏•‡∏¥‡∏õ‡∏Ñ‡πà‡∏≤‡∏•‡∏ö‡∏≠‡∏≠‡∏Å
    for k in scores:
        scores[k] = max(0.0, scores[k])

    return scores


def analyze_pose(keypoints) -> dict:
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ó‡πà‡∏≤‡∏ó‡∏≤‡∏á‡∏à‡∏≤‡∏Å keypoints ‡∏î‡πâ‡∏ß‡∏¢ multi-signal scoring

    Returns: {"behavior": str, "confidence": int, "details": dict}
    """
    if keypoints is None or len(keypoints) < 13:
        return {"behavior": "unknown", "confidence": 0, "details": {}}

    scores = _score_behavior(keypoints)
    total  = sum(scores.values())

    if total < 1.0:
        # signal ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (keypoints ‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏´‡∏ç‡πà confidence ‡∏ï‡πà‡∏≥ / ‡∏Ñ‡∏ô‡∏ñ‡∏π‡∏Å‡∏ö‡∏î‡∏ö‡∏±‡∏á‡∏°‡∏≤‡∏Å)
        return {"behavior": "unknown", "confidence": 0,
                "details": {"scores": {k: round(v, 2) for k, v in scores.items()}}}

    best  = max(scores, key=scores.get)
    best_score = scores[best]
    confidence = min(97, int(round((best_score / total) * 100)))

    # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ä‡∏ô‡∏∞‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÉ‡∏´‡πâ fallback ‡πÄ‡∏õ‡πá‡∏ô attentive (‡∏Å‡∏£‡∏ì‡∏µ ambiguous)
    sorted_vals = sorted(scores.values(), reverse=True)
    if len(sorted_vals) > 1 and best_score - sorted_vals[1] < 0.5 and best != "attentive":
        best       = "attentive"
        confidence = max(40, confidence - 15)

    visible_kp = int(sum(
        1 for kp in keypoints if len(kp) >= 3 and kp[2] >= KP_CONF_THRESHOLD
    ))

    return {
        "behavior":   best,
        "confidence": confidence,
        "details": {
            "scores":           {k: round(v, 2) for k, v in scores.items()},
            "visible_keypoints": visible_kp,
        },
    }


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Frame-level analysis
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def analyze_frame(frame: np.ndarray) -> dict:
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏ü‡∏£‡∏° ‡∏û‡∏£‡πâ‡∏≠‡∏° preprocessing ‡πÅ‡∏•‡∏∞ annotated output

    Args:
        frame: BGR numpy array
    Returns:
        dict: total_people, behaviors, summary, attention_rate, annotated_frame
    """
    processed = preprocess_frame(frame)

    model   = get_pose_model()
    results = model(
        processed,
        verbose=False,
        conf=0.35,   # detection confidence ‡∏ï‡πà‡∏≥‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏à‡∏≠‡∏ö‡∏î‡∏ö‡∏±‡∏á
        iou=0.45,    # NMS IoU ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô duplicate detection
        imgsz=640,
    )

    _empty = {
        "total_people": 0,
        "behaviors": [],
        "summary": {"attentive": 0, "sleeping": 0,
                    "looking_down": 0, "looking_away": 0, "unknown": 0},
        "attention_rate": 0,
        "annotated_frame": frame,
    }

    if not results or results[0].keypoints is None:
        return _empty

    keypoints_data = results[0].keypoints.data.cpu().numpy()
    boxes          = results[0].boxes

    if len(keypoints_data) == 0:
        return _empty

    behaviors      = []
    behavior_counts = {"attentive": 0, "sleeping": 0,
                       "looking_down": 0, "looking_away": 0, "unknown": 0}

    for kp in keypoints_data:
        analysis = analyze_pose(kp)
        behaviors.append(analysis)
        beh = analysis["behavior"]
        if beh in behavior_counts:
            behavior_counts[beh] += 1

    total_people    = len(behaviors)
    attentive_count = behavior_counts["attentive"]
    attention_rate  = round((attentive_count / total_people) * 100, 1) if total_people else 0

    # ‚îÄ‚îÄ Annotated frame ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    annotated_frame = results[0].plot(conf=False, labels=False)

    _COLOR = {
        "attentive":    ( 50, 205,  50),  # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß
        "sleeping":     (  0,   0, 220),  # ‡πÅ‡∏î‡∏á
        "looking_down": (  0, 140, 255),  # ‡∏™‡πâ‡∏°
        "looking_away": (  0, 200, 200),  # ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á
        "unknown":      (180, 180, 180),  # ‡πÄ‡∏ó‡∏≤
    }

    if boxes is not None and len(boxes) > 0:
        for box, beh in zip(boxes, behaviors):
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            color  = _COLOR.get(beh["behavior"], (180, 180, 180))
            label  = get_behavior_label_th(beh["behavior"])
            conf_t = beh["confidence"]

            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)

            text = f"{label} {conf_t}%"
            (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.55, 2)
            label_y = max(y1 - 5, th + 5)
            cv2.rectangle(annotated_frame,
                          (x1, label_y - th - 4), (x1 + tw + 4, label_y + 2),
                          color, cv2.FILLED)
            cv2.putText(annotated_frame, text, (x1 + 2, label_y),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255, 255, 255), 2)

    # ‚îÄ‚îÄ HUD bar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    h, w = annotated_frame.shape[:2]
    hud = (f"Attention {attention_rate}%  |  "
           f"Students: {total_people}  |  "
           f"Sleeping: {behavior_counts['sleeping']}  |  "
           f"Phone/Down: {behavior_counts['looking_down']}")
    cv2.rectangle(annotated_frame, (0, h - 32), (w, h), (30, 30, 30), cv2.FILLED)
    cv2.putText(annotated_frame, hud, (8, h - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.55, (220, 220, 220), 1)

    return {
        "total_people":    total_people,
        "behaviors":       behaviors,
        "summary":         behavior_counts,
        "attention_rate":  attention_rate,
        "annotated_frame": annotated_frame,
    }


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Label helpers
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def get_behavior_label_th(behavior: str) -> str:
    return {
        "attentive":    "‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏à‡πÄ‡∏£‡∏µ‡∏¢‡∏ô",
        "sleeping":     "‡∏´‡∏•‡∏±‡∏ö",
        "looking_down": "‡∏Å‡πâ‡∏°‡∏´‡∏ô‡πâ‡∏≤/‡πÇ‡∏ó‡∏£‡∏®‡∏±‡∏û‡∏ó‡πå",
        "looking_away": "‡∏°‡∏≠‡∏á‡∏≠‡∏≠‡∏Å",
        "unknown":      "‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö",
    }.get(behavior, behavior)


def get_behavior_label_en(behavior: str) -> str:
    return {
        "attentive":    "Attentive",
        "sleeping":     "Sleeping",
        "looking_down": "Phone/Looking Down",
        "looking_away": "Looking Away",
        "unknown":      "Unknown",
    }.get(behavior, behavior)
